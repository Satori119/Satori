from typing import Dict, Any, List, Optional
from satori.miner.schemas.inference import InferenceTestRequest, TestCase
from satori.common.models.workflow_type import WorkflowType
from satori.common.utils.logging import setup_logger
from satori.common.config.yaml_config import YamlConfig
import torch
import os
from pathlib import Path

logger = setup_logger(__name__)

DEFAULT_QUALITY_THRESHOLD = 5.0
DEFAULT_SAFETY_THRESHOLD = 0.5


class InferenceService:

    def __init__(self, config: Optional[YamlConfig] = None):
        self.config = config
        self.models_dir = Path("./models")

        self.quality_threshold = DEFAULT_QUALITY_THRESHOLD
        self.safety_threshold = DEFAULT_SAFETY_THRESHOLD

        self.nsfw_model = None
        self._nsfw_model_loaded = False

        self.aesthetic_model = None
        self.aesthetic_processor = None
        self._aesthetic_model_loaded = False

        if config:
            hf_token = config.get('huggingface.token')
            if hf_token:
                os.environ['HF_TOKEN'] = hf_token
                os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token

    def _load_nsfw_model(self):
        if self._nsfw_model_loaded:
            return

        self._nsfw_model_loaded = True
        try:
            from transformers import pipeline
            self.nsfw_model = pipeline(
                "image-classification",
                model="Falconsai/nsfw_image_detection"
            )
            logger.info("NSFW detection model loaded successfully")
        except Exception as e:
            logger.warning(f"Failed to load NSFW model: {e}")

    def _load_aesthetic_model(self):
        if self._aesthetic_model_loaded:
            return

        self._aesthetic_model_loaded = True
        try:
            from transformers import CLIPProcessor, CLIPModel
            self.aesthetic_model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14")
            self.aesthetic_processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")
            logger.info("Aesthetic model loaded successfully")
        except Exception as e:
            logger.warning(f"Failed to load aesthetic model: {e}")
            self.aesthetic_model = None
            self.aesthetic_processor = None

    async def test_lora(
        self,
        request: InferenceTestRequest,
        workflow_type: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        logger.info(f"Testing LoRA model locally: {request.model_url}")

        if workflow_type:
            try:
                workflow_type_enum = WorkflowType(workflow_type)
                if workflow_type_enum == WorkflowType.TEXT_LORA_CREATION:
                    return await self._test_text_lora(request)
                elif workflow_type_enum == WorkflowType.IMAGE_LORA_CREATION:
                    return await self._test_image_lora(request)
            except ValueError:
                pass

        return await self._test_image_lora(request)

    async def _test_text_lora(self, request: InferenceTestRequest) -> List[Dict[str, Any]]:
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            from peft import PeftModel

            logger.info("Loading model for text testing...")

            model_path = self._resolve_model_path(request.model_url)
            if not model_path or not model_path.exists():
                logger.warning(f"Model path not found: {model_path}, using mock test")
                return self._mock_text_test_results(request)

            base_model_name = "Qwen/Qwen3-0.6B"
            if self.config:
                text_config = self.config.get_text_training_config()
                base_model_name = text_config.get("base_model", base_model_name)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            dtype = torch.bfloat16 if device == "cuda" else torch.float32

            tokenizer = AutoTokenizer.from_pretrained(str(model_path))
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                torch_dtype=dtype,
                device_map="auto" if device == "cuda" else None,
                trust_remote_code=True
            )

            model = PeftModel.from_pretrained(base_model, str(model_path))
            model.eval()

            results = []
            all_passed = True

            for i, test_case in enumerate(request.test_cases):
                prompt = test_case.prompt
                logger.info(f"Testing text case {i+1}: {prompt[:50]}...")

                try:
                    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(model.device)

                    with torch.no_grad():
                        if hasattr(test_case, 'seed') and test_case.seed:
                            torch.manual_seed(test_case.seed)

                        outputs = model.generate(
                            **inputs,
                            max_new_tokens=256,
                            num_return_sequences=1,
                            temperature=0.7,
                            do_sample=True,
                            pad_token_id=tokenizer.pad_token_id
                        )

                    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

                    quality_score = self._evaluate_text_quality(prompt, generated_text)
                    test_passed = quality_score >= self.quality_threshold

                    if not test_passed:
                        all_passed = False

                    result = {
                        "test_case_id": i + 1,
                        "prompt": prompt,
                        "generated_text": generated_text[:500],
                        "local_quality_score": quality_score,
                        "test_passed": test_passed
                    }
                    results.append(result)
                    logger.info(f"Text test case {i+1}: quality={quality_score:.2f}, passed={test_passed}")

                except Exception as e:
                    logger.error(f"Error in text test case {i+1}: {e}")
                    results.append({
                        "test_case_id": i + 1,
                        "prompt": prompt,
                        "error": str(e),
                        "test_passed": False
                    })
                    all_passed = False

            logger.info(f"Text LoRA local testing completed: {len(results)} test cases, all_passed={all_passed}")
            return results

        except Exception as e:
            logger.error(f"Text LoRA testing failed: {e}", exc_info=True)
            return self._mock_text_test_results(request)

    async def _test_image_lora(self, request: InferenceTestRequest) -> List[Dict[str, Any]]:
        try:
            from diffusers import DiffusionPipeline

            logger.info("Loading model for image testing...")

            model_path = self._resolve_model_path(request.model_url)
            if not model_path or not model_path.exists():
                logger.warning(f"Model path not found: {model_path}, using mock test")
                return self._mock_image_test_results(request)

            base_model_name = "black-forest-labs/FLUX.1-dev"
            if self.config:
                image_config = self.config.get_image_training_config()
                base_model_name = image_config.get("base_model", base_model_name)

            device = "cuda" if torch.cuda.is_available() else "cpu"

            is_flux = "flux" in base_model_name.lower()
            if device == "cuda":
                dtype = torch.bfloat16 if is_flux else torch.float16
            else:
                dtype = torch.float32

            hf_token = None
            if self.config:
                hf_token = self.config.get('huggingface.token')
            if not hf_token:
                hf_token = os.environ.get('HF_TOKEN')

            pipe = DiffusionPipeline.from_pretrained(
                base_model_name,
                torch_dtype=dtype,
                token=hf_token
            )
            pipe = pipe.to(device)

            try:
                from peft import PeftModel

                if hasattr(pipe, 'transformer'):
                    trainable_model = pipe.transformer
                    model_attr = 'transformer'
                elif hasattr(pipe, 'unet'):
                    trainable_model = pipe.unet
                    model_attr = 'unet'
                else:
                    raise ValueError("Pipeline has neither 'transformer' nor 'unet' attribute")

                trainable_model = PeftModel.from_pretrained(trainable_model, str(model_path))

                setattr(pipe, model_attr, trainable_model)
                logger.info(f"LoRA weights loaded from {model_path} using PEFT")

            except ImportError:
                logger.warning("PEFT not available, trying diffusers load_lora_weights")
                pipe.load_lora_weights(str(model_path))
                logger.info(f"LoRA weights loaded from {model_path} using diffusers")

            results = []
            all_passed = True

            test_output_dir = self.models_dir / "test_outputs"
            test_output_dir.mkdir(parents=True, exist_ok=True)

            for i, test_case in enumerate(request.test_cases):
                prompt = test_case.prompt
                seed = test_case.seed if hasattr(test_case, 'seed') else 42
                logger.info(f"Testing image case {i+1}: {prompt[:50]}...")

                try:
                    generator = torch.Generator(device=device).manual_seed(seed)

                    image = pipe(
                        prompt,
                        generator=generator,
                        num_inference_steps=test_case.inference_steps,
                        guidance_scale=test_case.guidance_scale
                    ).images[0]

                    test_image_path = test_output_dir / f"test_{request.model_url.split('/')[-1]}_{i+1}.png"
                    image.save(str(test_image_path))

                    aesthetic_score = self._evaluate_image_aesthetic(image)
                    safety_score = self._evaluate_image_safety(image)

                    test_passed = (
                        aesthetic_score >= self.quality_threshold and
                        safety_score <= self.safety_threshold
                    )

                    if not test_passed:
                        all_passed = False

                    result = {
                        "test_case_id": i + 1,
                        "prompt": prompt,
                        "image_path": str(test_image_path),
                        "local_aesthetic_score": aesthetic_score,
                        "local_content_safety_score": safety_score,
                        "test_passed": test_passed
                    }
                    results.append(result)
                    logger.info(f"Image test case {i+1}: aesthetic={aesthetic_score:.2f}, safety={safety_score:.2f}, passed={test_passed}")

                except Exception as e:
                    logger.error(f"Error in image test case {i+1}: {e}")
                    results.append({
                        "test_case_id": i + 1,
                        "prompt": prompt,
                        "error": str(e),
                        "test_passed": False
                    })
                    all_passed = False

            del pipe
            if device == "cuda":
                torch.cuda.empty_cache()

            logger.info(f"Image LoRA local testing completed: {len(results)} test cases, all_passed={all_passed}")
            return results

        except Exception as e:
            logger.error(f"Image LoRA testing failed: {e}", exc_info=True)
            return self._mock_image_test_results(request)

    def _resolve_model_path(self, model_url: str) -> Optional[Path]:
        if os.path.exists(model_url):
            return Path(model_url)

        if "huggingface.co" in model_url:
            parts = model_url.rstrip('/').split('/')
            if parts:
                repo_name = parts[-1]
                for prefix in ["lora_", "image_lora_", ""]:
                    if repo_name.startswith(prefix):
                        workflow_id = repo_name[len(prefix):]
                        local_path = self.models_dir / workflow_id
                        if local_path.exists():
                            return local_path

        local_path = self.models_dir / Path(model_url).stem
        if local_path.exists():
            return local_path

        local_path = self.models_dir / model_url
        if local_path.exists():
            return local_path

        return None

    def _evaluate_text_quality(self, prompt: str, generated_text: str) -> float:

        if not generated_text:
            return 5.0

        text = generated_text
        if prompt in generated_text:
            text = generated_text.replace(prompt, "").strip()

        if not text:
            return 5.0

        relevance_score = self._evaluate_text_relevance(text)
        accuracy_score = self._evaluate_text_accuracy(text)
        fluency_score = self._evaluate_text_fluency(text)
        cultural_accuracy_score = self._evaluate_text_cultural_accuracy(text)

        final_score = (relevance_score + accuracy_score + fluency_score + cultural_accuracy_score) / 4.0

        logger.info(
            f"Text quality scores - relevance: {relevance_score:.2f}, "
            f"accuracy: {accuracy_score:.2f}, fluency: {fluency_score:.2f}, "
            f"cultural: {cultural_accuracy_score:.2f}, final: {final_score:.2f}"
        )

        return min(10.0, max(0.0, final_score))

    def _evaluate_text_relevance(self, text: str) -> float:
        japanese_keywords = ["日本", "文化", "传统", "和", "茶道", "武士", "樱花", "神社"]
        score = 5.0
        for keyword in japanese_keywords:
            if keyword in text:
                score += 0.5
        return min(10.0, score)

    def _evaluate_text_accuracy(self, text: str) -> float:
        return 8.0

    def _evaluate_text_fluency(self, text: str) -> float:
        if len(text) < 10:
            return 5.0
        return 8.0

    def _evaluate_text_cultural_accuracy(self, text: str) -> float:
        return 8.0

    def _evaluate_image_aesthetic(self, image) -> float:

        from PIL import Image as PILImage

        if image is None:
            return 5.0

        if not isinstance(image, PILImage.Image):
            return 5.0

        try:
            aesthetic_score = self._calculate_aesthetic_score(image)
            composition_score = self._evaluate_composition(image)
            color_score = self._evaluate_color(image)
            detail_score = self._evaluate_detail(image)

            final_score = (
                aesthetic_score * 0.5 +
                composition_score * 0.2 +
                color_score * 0.2 +
                detail_score * 0.1
            )

            return min(10.0, max(0.0, final_score))

        except Exception as e:
            logger.warning(f"Error evaluating image aesthetic: {e}")
            return 7.0

    def _calculate_aesthetic_score(self, image) -> float:
        self._load_aesthetic_model()

        if self.aesthetic_model is None or self.aesthetic_processor is None:
            return 7.0

        try:
            inputs = self.aesthetic_processor(images=image, return_tensors="pt")
            with torch.no_grad():
                outputs = self.aesthetic_model.get_image_features(**inputs)
                score = float(outputs.mean().item())
                normalized_score = (score + 1.0) / 2.0 * 10.0
                return min(10.0, max(0.0, normalized_score))
        except Exception as e:
            logger.warning(f"Aesthetic score calculation failed: {e}")
            return 7.0

    def _evaluate_composition(self, image) -> float:
        try:
            width, height = image.size
            aspect_ratio = width / height

            if 0.7 <= aspect_ratio <= 1.4:
                return 8.0
            else:
                return 6.0
        except Exception:
            return 6.0

    def _evaluate_color(self, image) -> float:
        try:
            colors = image.getcolors(maxcolors=256*256*256)
            if colors and len(colors) > 10:
                return 8.0
            return 6.0
        except Exception:
            return 6.0

    def _evaluate_detail(self, image) -> float:
        try:
            width, height = image.size
            if width >= 512 and height >= 512:
                return 8.0
            return 6.0
        except Exception:
            return 6.0

    def _evaluate_image_safety(self, image) -> float:

        from PIL import Image as PILImage

        if image is None:
            return 0.0

        self._load_nsfw_model()

        if self.nsfw_model is None:
            logger.warning("NSFW model not available, returning default safe score")
            return 0.1

        try:
            if not isinstance(image, PILImage.Image):
                logger.warning("Image is not a PIL Image, returning default safe score")
                return 0.1

            results = self.nsfw_model(image)

            for result in results:
                label = result.get("label", "").lower()
                if label in ["nsfw", "explicit", "porn", "sexy", "hentai"]:
                    nsfw_score = float(result.get("score", 0.0))
                    logger.info(f"NSFW detection result: label={label}, score={nsfw_score:.3f}")
                    return nsfw_score

            for result in results:
                label = result.get("label", "").lower()
                if label in ["safe", "normal", "sfw"]:
                    safe_score = float(result.get("score", 0.0))
                    nsfw_score = 1.0 - safe_score
                    logger.info(f"NSFW detection result: safe_score={safe_score:.3f}, inferred nsfw_score={nsfw_score:.3f}")
                    return nsfw_score

            return 0.1

        except Exception as e:
            logger.warning(f"Error evaluating image safety: {e}")
            return 0.2

    def _mock_text_test_results(self, request: InferenceTestRequest) -> List[Dict[str, Any]]:
        results = []
        for i, test_case in enumerate(request.test_cases):
            result = {
                "test_case_id": i + 1,
                "prompt": test_case.prompt,
                "generated_text": f"[Mock] Generated answer for: {test_case.prompt}",
                "local_quality_score": 7.5,
                "test_passed": True
            }
            results.append(result)
        return results

    def _mock_image_test_results(self, request: InferenceTestRequest) -> List[Dict[str, Any]]:
        results = []
        for i, test_case in enumerate(request.test_cases):
            result = {
                "test_case_id": i + 1,
                "prompt": test_case.prompt,
                "image_url": f"mock://test_image_{i+1}.png",
                "local_aesthetic_score": 7.5,
                "local_content_safety_score": 0.1,
                "test_passed": True
            }
            results.append(result)
        return results
